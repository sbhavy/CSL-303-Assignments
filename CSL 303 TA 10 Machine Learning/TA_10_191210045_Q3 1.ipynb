{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "TA_10_191210045_Q3ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "854eb9c3"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid (x): # activation function\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "X = np.array([[0.6, 0.1], [0.2, 0.3]]) # test data\n",
        "Y = np.array([[1.0, 0.0], [0.0, 1.0]]) # output labels\n",
        "\n",
        "epoch = 50 # number of epochs for which backprop is to be run\n",
        "lr = 0.1   # learning rate\n",
        "\n",
        "W_h = np.array([[0.1, 0.0, 0.3], [-0.2, 0.2, -0.4]])    # weights in hidden layer\n",
        "B_h = np.array([0.1, 0.2, 0.5])                         # bias in hidden layer\n",
        "W_o = np.array([[-0.4, 0.2], [0.1, -0.1], [0.6, -0.2]]) # weights in output layer\n",
        "B_o = np.array([-0.1, 0.6])                             # bias in output layer"
      ],
      "id": "854eb9c3",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d57b888",
        "outputId": "5ed9bb21-2eb5-49cc-81b2-695c697aff16"
      },
      "source": [
        "for e in range(epoch):\n",
        "    \n",
        "    for i in range(X.shape[0]):\n",
        "        \n",
        "        h_temp = np.dot(X[i],W_h) + B_h  # forward pass from input -> hidden\n",
        "        h_act = sigmoid(h_temp)          # activate hidden-layer neurons\n",
        "    \n",
        "        o_temp = np.dot(h_act,W_o) + B_o # forward pass from hidden -> output\n",
        "        output = sigmoid(o_temp)         # activate output-layer neurons\n",
        "        \n",
        "        del_out = (Y[i] - output) * output * (1 - output)          # error in output\n",
        "        dW_o = lr * np.array([h_act]).T.dot(np.array([del_out]))   # find delta(Weight_output)\n",
        "        dB_o = lr * del_out * 1                                    # find delta(Bias_output)\n",
        "        \n",
        "        del_hidden = h_act * (1 - h_act) * W_o.dot(del_out.T)      # error in internal activation\n",
        "        dW_h = lr * np.array([X[i]]).T.dot(np.array([del_hidden])) # find delta(Weight_hidden)\n",
        "        dB_h = lr * del_hidden * 1                                 # find delta(Bias_hidden)\n",
        "        \n",
        "        # update the weights and biases accordingly\n",
        "        W_o += dW_o\n",
        "        B_o += dB_o\n",
        "        W_h += dW_h\n",
        "        B_h += dB_h\n",
        "    \n",
        "    if e in [0,1,49]: # print parameters for 1st, 2nd and 50th iterations\n",
        "        print(\"Iteration: \", e+1,\"\\n\")\n",
        "        print(\"Weights in hidden layer: \\n\" , str(W_h),\"\\n\")\n",
        "        print(\"Biases in hidden layer: \\n\" , str(B_h),\"\\n\")\n",
        "        print(\"Weights in output layer: \\n\" ,str(W_o),\"\\n\")\n",
        "        print(\"Biases in output layer: \\n\" , str(B_o),\"\\n\\n\\n\")"
      ],
      "id": "5d57b888",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:  1 \n",
            "\n",
            "Weights in hidden layer: \n",
            " [[ 9.92162570e-02  2.72431456e-04  3.00868637e-01]\n",
            " [-1.99667657e-01  1.99889457e-01 -4.00487120e-01]] \n",
            "\n",
            "Biases in hidden layer: \n",
            " [0.09985118 0.20006418 0.499868  ] \n",
            "\n",
            "Weights in output layer: \n",
            " [[-0.40063383  0.19673691]\n",
            " [ 0.09892641 -0.10310539]\n",
            " [ 0.59950097 -0.20417922]] \n",
            "\n",
            "Biases in output layer: \n",
            " [-0.1016944   0.59424127] \n",
            "\n",
            "\n",
            "\n",
            "Iteration:  2 \n",
            "\n",
            "Weights in hidden layer: \n",
            " [[ 0.09843748  0.00054864  0.30174373]\n",
            " [-0.19933512  0.19977811 -0.40097576]] \n",
            "\n",
            "Biases in hidden layer: \n",
            " [0.09970904 0.20013109 0.49974026] \n",
            "\n",
            "Weights in output layer: \n",
            " [[-0.40125028  0.19353339]\n",
            " [ 0.09787272 -0.10614852]\n",
            " [ 0.59902536 -0.20829079]] \n",
            "\n",
            "Biases in output layer: \n",
            " [-0.10335376  0.58859417] \n",
            "\n",
            "\n",
            "\n",
            "Iteration:  50 \n",
            "\n",
            "Weights in hidden layer: \n",
            " [[ 0.06519173  0.01597844  0.34831635]\n",
            " [-0.18368757  0.19330999 -0.42661493]] \n",
            "\n",
            "Biases in hidden layer: \n",
            " [0.09727073 0.20324803 0.49385811] \n",
            "\n",
            "Weights in output layer: \n",
            " [[-0.41708125  0.09743366]\n",
            " [ 0.06374033 -0.19230297]\n",
            " [ 0.59581789 -0.3401919 ]] \n",
            "\n",
            "Biases in output layer: \n",
            " [-0.15446236  0.42535047] \n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}